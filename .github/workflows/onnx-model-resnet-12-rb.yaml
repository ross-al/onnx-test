apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "onnx-model-resnet-12-rb"
  namespace: "onnx-model-resnet-12-rb"
spec:
  predictor:
    model:
      modelFormat:
        name: onnx
      protocolVersion: v2
      storageUri: "s3://mlflow-lite-bucket//mlflow-artifacts/3/9c67eb39121b42a8a0537c838f410023/artifacts/model"
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "1"
          memory: "2Gi"
    minReplicas: 1
    maxReplicas: 5
    autoscaler:
      targetCpuUtilizationPercentage: 80